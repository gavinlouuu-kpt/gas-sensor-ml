{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kedro.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext kedro.ipython\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import yaml\n",
    "\n",
    "from typing import Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "%load_ext kedro.ipython\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write functions to check if each node produces valid data that are suitable for next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to combine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data between exp_no 104 to 113\n",
    "# append data of 105 at the end of 104 etc\n",
    "\n",
    "def get_data(exp_no) -> pd.DataFrame:\n",
    "    file_name = f\"{exp_no}_SHT_SMD.txt\"\n",
    "    file_path = f\"../data/01_raw/{file_name}\"\n",
    "    df = pd.read_csv(file_path, sep=',', usecols=['timestamp', 'SHT40_temp', 'SHT40_Humidity', 'A1_Sensor', 'A1_Resistance'])\n",
    "    return df\n",
    "\n",
    "def concat_data(start:int,end:int) -> pd.DataFrame:\n",
    "    df = pd.concat([get_data(exp_no) for exp_no in range(start, end)])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "s_file = 108\n",
    "e_file = 113\n",
    "\n",
    "concat_data = concat_data(s_file,e_file)\n",
    "# concat_data(s_file,e_file).to_parquet(f'../data/02_intermediate/{s_file}_{e_file}.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hi_lo_peak(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    peaks, properties = find_peaks(x['A1_Sensor'], width=50, height=1)\n",
    "    peak_heights = properties['peak_heights']\n",
    "# Determine smaller and larger peaks\n",
    "    smaller_peaks, larger_peaks = [], []\n",
    "    for i in range(len(peaks) - 1):\n",
    "        if peak_heights[i] > peak_heights[i + 1]:\n",
    "            larger_peaks.append(peaks[i])\n",
    "            smaller_peaks.append(peaks[i + 1])\n",
    "    # smaller_peaks_df = x.iloc[smaller_peaks]\n",
    "    return smaller_peaks\n",
    "\n",
    "def data_stack(sp: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    After finding the peaks, stack the data according to exp_no\n",
    "    \"\"\"\n",
    "    df_stacked_list = []\n",
    "    for i in range(len(sp) - 1):\n",
    "        df_subset = df.iloc[sp[i]:sp[i + 1]].copy()\n",
    "        df_subset['exp_no'] = i\n",
    "        df_subset['timestamp'] -= df_subset['timestamp'].iloc[0]\n",
    "        df_stacked_list.append(df_subset)\n",
    "        df_stacked = pd.concat(df_stacked_list, ignore_index=True)\n",
    "    return df_stacked\n",
    "\n",
    "\n",
    "def _group_by_bin(df_stacked: pd.DataFrame, num_bins: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use PD.CUT to group data into specified bins in parameters\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    grouped = df_stacked.groupby('exp_no')\n",
    "    for name, group in grouped:\n",
    "        group['bin'] = pd.cut(group['timestamp'], bins=num_bins, labels=False)\n",
    "        df_list.append(group)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def _average_bin(bin_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    average values within each bin to return only one data point\n",
    "    \"\"\"\n",
    "    bin_df = bin_df.drop(columns=['timestamp'])\n",
    "    grouped = bin_df.groupby(['exp_no', 'bin']).mean()\n",
    "    return grouped.reset_index()\n",
    "\n",
    "def preprocess_data_bin(mox: pd.DataFrame, num_bins: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return data that is sorted by experiment number according to lo_peak interval\n",
    "    data is stacked and labeled by exp_no\n",
    "    data is grouped by bin and averaged\n",
    "    \"\"\"\n",
    "    df_stacked = data_stack(_hi_lo_peak(mox), mox)\n",
    "    bin_df = _group_by_bin(df_stacked, num_bins)\n",
    "    mean_bin = _average_bin(bin_df)\n",
    "    return mean_bin\n",
    "\n",
    "def get_percentile_data(df, percentile):\n",
    "    \"\"\"\n",
    "    Returns the data up to the specified percentile based on the 'bin' column.\n",
    "\n",
    "    :param df: DataFrame containing the data\n",
    "    :param percentile: A float value between 0 and 1 representing the percentile\n",
    "    :return: DataFrame containing the data up to the specified percentile\n",
    "    \"\"\"\n",
    "    # Calculate the bin index corresponding to the percentile\n",
    "    max_bin = int(percentile * df['bin'].max())\n",
    "\n",
    "    # Return data up to that bin\n",
    "    return df[df['bin'] <= max_bin]\n",
    "\n",
    "def _group_percentile (averaged: pd.DataFrame, percentile_bins: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns the full specified percentile dataset\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    grouped = averaged.groupby('exp_no')\n",
    "    for name, group in grouped:\n",
    "        percentile_data = get_percentile_data(group, percentile_bins)\n",
    "        df_list.append(percentile_data)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def _transpose_(df_set: pd.DataFrame, target: str) -> pd.DataFrame:\n",
    "    transposed = df_set.pivot(index='exp_no', columns='bin', values=target)\n",
    "    transposed.columns = ['bin_' + str(col) for col in transposed.columns]\n",
    "    transposed.reset_index(inplace=True)\n",
    "    return transposed\n",
    "\n",
    "\n",
    "def _res_ratio(averaged: pd.DataFrame) -> pd.DataFrame:\n",
    "    def calculate_res_ratio(group):\n",
    "        return group['A1_Resistance'].max() / group['A1_Resistance'].min()\n",
    "\n",
    "    res_ratio = averaged.groupby('exp_no').apply(calculate_res_ratio).reset_index()\n",
    "    res_ratio.columns = ['exp_no', 'res_ratio']\n",
    "    return res_ratio\n",
    "\n",
    "def _combine_feature_matrix(res_ratio: pd.DataFrame, transposed: pd.DataFrame) -> pd.DataFrame:\n",
    "    combined = pd.merge(res_ratio, transposed, on='exp_no')\n",
    "    return combined\n",
    "\n",
    "def create_model_input_table(mox_bin: pd.DataFrame, percentile_bins: float) -> pd.DataFrame:\n",
    "    selected_range = _group_percentile(mox_bin, percentile_bins)\n",
    "    # the ratio is from the entire dataset not filtered to be ground truth\n",
    "    res_ratio = _res_ratio(mox_bin) \n",
    "    transpose_col = _transpose_(selected_range)\n",
    "    # drop exp_no to avoid training on exp_no\n",
    "    mox_table = _combine_feature_matrix(transpose_col, res_ratio).drop(columns=['exp_no'])\n",
    "    return mox_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "with open('nb_parameters.yml') as file:\n",
    "    parameters = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "test_size = parameters['model_options']['test_size']\n",
    "\n",
    "print(test_size)\n",
    "\n",
    "\n",
    "# Hyper-parameters \n",
    "\n",
    "num_classes = parameters['model_options']['num_classes']\n",
    "num_epochs = parameters['model_options']['num_epochs']\n",
    "batch_size = parameters['model_options']['batch_size']\n",
    "learning_rate = parameters['model_options']['learning_rate']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Each feature as a time step in your sequence, you could set sequence_length to 150 and input_size to 1.\n",
    "This would mean you are feeding in sequences of length 150, with each time step in the sequence having 1 feature.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_size = int(parameters['model_options']['input_size'])\n",
    "# sequence_length = parameters['model_options']['sequence_length'] # the window it trains with can be selected\n",
    "hidden_size = parameters['model_options']['hidden_size']\n",
    "num_layers = parameters['model_options']['num_layers']\n",
    "random_state = parameters['model_options']['random_state']\n",
    "val_size = parameters['model_options']['val_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Process and examine each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporting NaN value in dataset\n",
    "    # print(pd.DataFrame(X_test_tensor.numpy()).isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_no = 107\n",
    "percentile_bins = parameters['percentile_bins']\n",
    "bin_size = int(parameters['num_bins'])\n",
    "sequence_length = int(percentile_bins*bin_size)\n",
    "target = parameters['target']\n",
    "\n",
    "df_exp = get_data(exp_no)\n",
    "smaller_peaks = _hi_lo_peak(df_exp)\n",
    "df_stacked = data_stack(smaller_peaks, df_exp)\n",
    "bin_df = _group_by_bin(df_stacked, bin_size)\n",
    "mean_bin = _average_bin(bin_df)\n",
    "mox_bin = preprocess_data_bin(df_exp, bin_size)\n",
    "selected_range = _group_percentile(mox_bin, percentile_bins)\n",
    "res_ratio = _res_ratio(mox_bin)\n",
    "transpose_col = _transpose_(selected_range, target) # target can be changed to 'A1_Resistance' or 'SHT40_temp'\n",
    "mox_table = _combine_feature_matrix(transpose_col, res_ratio).drop(columns=['exp_no'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search within selected_range for NaN values\n",
    "print(\"Original Data\")\n",
    "print(selected_range.isna().sum())\n",
    "print(\"Transposed Data\")\n",
    "print(transpose_col.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LSTM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN processing\n",
    "def _clean_NaN (X_dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_dataset_df = pd.DataFrame(X_dataset, columns=mox_table.columns[:-1])\n",
    "    # Fill NaN values with the mean of the column\n",
    "    X_dataset_df.fillna(X_dataset_df.mean(), inplace=True)\n",
    "    # Convert back to numpy arrays\n",
    "    X_dataset = X_dataset_df.values\n",
    "    return X_dataset\n",
    "\n",
    "# drop the column with NaN values\n",
    "def _drop_NaN (X_dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_dataset_df = pd.DataFrame(X_dataset, columns=mox_table.columns[:-1])\n",
    "    # Fill NaN values with the mean of the column\n",
    "    X_dataset_df.dropna(inplace=True)\n",
    "    # Convert back to numpy arrays\n",
    "    X_dataset = X_dataset_df.values\n",
    "    return X_dataset\n",
    "\n",
    "# forward fill NaN values\n",
    "def _ffill_NaN (X_dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_dataset_df = pd.DataFrame(X_dataset, columns=mox_table.columns[:-1])\n",
    "    # Fill NaN values with the mean of the column\n",
    "    X_dataset_df.ffill(inplace=True)\n",
    "    # Convert back to numpy arrays\n",
    "    X_dataset = X_dataset_df.values\n",
    "    return X_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement LSTM functions below\n",
    "# there is no validation set in this example\n",
    "# load mox_table as input\n",
    "\n",
    "def split_data(model_input_table: pd.DataFrame) -> torch.tensor:\n",
    "    # Split data into features and target\n",
    "    X = model_input_table[model_input_table.columns[:-1]].values  # Assuming last column is the target\n",
    "    y = model_input_table[model_input_table.columns[-1]].values\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size = test_size, random_state = random_state)\n",
    "    \n",
    "    # Further split to create a validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, \n",
    "        test_size = val_size, random_state = random_state)\n",
    "    \n",
    "    # Clean NaN values\n",
    "    X_train = _ffill_NaN(X_train)\n",
    "    X_val = _ffill_NaN(X_val)\n",
    "    X_test = _ffill_NaN(X_test)\n",
    "\n",
    "    X_val_df = pd.DataFrame(X_val, columns=model_input_table.columns[:-1])\n",
    "    # Fill NaN values with the mean of the column\n",
    "    X_val_df.fillna(X_val_df.mean(), inplace=True)\n",
    "    # Convert back to numpy arrays\n",
    "    X_val = X_val_df.values\n",
    "\n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training data\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # Transform both training and testing data\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Ensure y_train and y_test are in the correct format\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train = y_train.values\n",
    "    if isinstance(y_val, pd.Series):\n",
    "        y_val = y_val.values\n",
    "    if isinstance(y_test, pd.Series):\n",
    "        y_test = y_test.values\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled.astype(np.float32))\n",
    "    y_train_tensor = torch.tensor(y_train.astype(np.float32))\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val_scaled.astype(np.float32))\n",
    "    y_val_tensor = torch.tensor(y_val.astype(np.float32))\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test_scaled.astype(np.float32))\n",
    "    y_test_tensor = torch.tensor(y_test.astype(np.float32))\n",
    "\n",
    "    return X_train_tensor, X_val_tensor, X_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor\n",
    "\n",
    "\n",
    "# create X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor from split_data(df)\n",
    "X_train_tensor, X_val_tensor, X_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor = split_data(mox_table)\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "# Initialize DataLoaders\n",
    "batch_size = parameters['model_options']['batch_size']  # You can adjust the batch size according to your needs\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# turn the block below into a function\n",
    "def train_model (data: DataLoader)->():\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "    n_total_steps = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (bins, target) in enumerate(train_loader):  \n",
    "            bins = bins.reshape(-1, sequence_length, input_size).to(device)\n",
    "            target = target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(bins)\n",
    "        # Example of reshaping/squeezing if applicable\n",
    "        outputs = outputs.squeeze()  # Removes dimensions of size 1\n",
    "        outputs = outputs[:64]  # Adjust if you need to slice the outputs\n",
    "\n",
    "        target = target.unsqueeze(1).to(device)  # Add an extra dimension to match outputs\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Calculate RMSE at the end of each epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Don't calculate gradients\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for bins, target in val_loader:  # Replace with your validation loader\n",
    "                bins = bins.reshape(-1, sequence_length, input_size).to(device)\n",
    "                target = target.unsqueeze(1).to(device)  # Add an extra dimension to match outputs\n",
    "                outputs = model(bins)\n",
    "                loss = criterion(outputs, target)\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "            rmse = np.sqrt(total_loss / count)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], RMSE on validation data: {rmse}, Total loss: {total_loss}, Count: {count}')\n",
    "        model.train()  # Set the model back to training mode\n",
    "    # Save the model after training\n",
    "    # lstm_model = torch.save(model.state_dict())\n",
    "    lstm_model = model.state_dict()\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], RMSE on validation data: 0.053660214199469834, Total loss: 0.008638255763798952, Count: 3\n",
      "Epoch [2/100], RMSE on validation data: 0.05694760792962952, Total loss: 0.00972909014672041, Count: 3\n",
      "Epoch [3/100], RMSE on validation data: 0.033081135040035466, Total loss: 0.0032830844866111875, Count: 3\n",
      "Epoch [4/100], RMSE on validation data: 0.03622336548280221, Total loss: 0.0039363966207019985, Count: 3\n",
      "Epoch [5/100], RMSE on validation data: 0.036152750122802835, Total loss: 0.00392106402432546, Count: 3\n",
      "Epoch [6/100], RMSE on validation data: 0.027064252031524422, Total loss: 0.0021974212140776217, Count: 3\n",
      "Epoch [7/100], RMSE on validation data: 0.022295603348360102, Total loss: 0.0014912817860022187, Count: 3\n",
      "Epoch [8/100], RMSE on validation data: 0.02873608656564159, Total loss: 0.0024772880133241415, Count: 3\n",
      "Epoch [9/100], RMSE on validation data: 0.023216918223582107, Total loss: 0.0016170758754014969, Count: 3\n",
      "Epoch [10/100], RMSE on validation data: 0.020943620217471856, Total loss: 0.0013159056834410876, Count: 3\n",
      "Epoch [11/100], RMSE on validation data: 0.01838749453067131, Total loss: 0.001014299865346402, Count: 3\n",
      "Epoch [12/100], RMSE on validation data: 0.018298039933305067, Total loss: 0.0010044547962024808, Count: 3\n",
      "Epoch [13/100], RMSE on validation data: 0.019958352944874677, Total loss: 0.001195007556816563, Count: 3\n",
      "Epoch [14/100], RMSE on validation data: 0.02234067957288613, Total loss: 0.0014973178913351148, Count: 3\n",
      "Epoch [15/100], RMSE on validation data: 0.021991125045010763, Total loss: 0.001450828742235899, Count: 3\n",
      "Epoch [16/100], RMSE on validation data: 0.019851196853858784, Total loss: 0.0011822100495919585, Count: 3\n",
      "Epoch [17/100], RMSE on validation data: 0.020452389255369335, Total loss: 0.0012549006787594408, Count: 3\n",
      "Epoch [18/100], RMSE on validation data: 0.019839789521066936, Total loss: 0.0011808517447207123, Count: 3\n",
      "Epoch [19/100], RMSE on validation data: 0.01928019575756027, Total loss: 0.0011151778453495353, Count: 3\n",
      "Epoch [20/100], RMSE on validation data: 0.02269779173006819, Total loss: 0.0015455692482646555, Count: 3\n",
      "Epoch [21/100], RMSE on validation data: 0.026053515113111703, Total loss: 0.00203635694924742, Count: 3\n",
      "Epoch [22/100], RMSE on validation data: 0.024951223788919705, Total loss: 0.0018676907056942582, Count: 3\n",
      "Epoch [23/100], RMSE on validation data: 0.02644423461534111, Total loss: 0.0020978926331736147, Count: 3\n",
      "Epoch [24/100], RMSE on validation data: 0.02539175746299589, Total loss: 0.0019342240411788225, Count: 3\n",
      "Epoch [25/100], RMSE on validation data: 0.02211494271756404, Total loss: 0.0014672120742034167, Count: 3\n",
      "Epoch [26/100], RMSE on validation data: 0.029696876506238125, Total loss: 0.0026457134226802737, Count: 3\n",
      "Epoch [27/100], RMSE on validation data: 0.0417128001901982, Total loss: 0.0052198730991221964, Count: 3\n",
      "Epoch [28/100], RMSE on validation data: 0.052506671963605, Total loss: 0.00827085180208087, Count: 3\n",
      "Epoch [29/100], RMSE on validation data: 0.048632433470432467, Total loss: 0.00709534075576812, Count: 3\n",
      "Epoch [30/100], RMSE on validation data: 0.03530989210950303, Total loss: 0.003740365442354232, Count: 3\n",
      "Epoch [31/100], RMSE on validation data: 0.023591052854409093, Total loss: 0.0016696133243385702, Count: 3\n",
      "Epoch [32/100], RMSE on validation data: 0.023819976531212064, Total loss: 0.0017021738458424807, Count: 3\n",
      "Epoch [33/100], RMSE on validation data: 0.029846498569709966, Total loss: 0.0026724404306150973, Count: 3\n",
      "Epoch [34/100], RMSE on validation data: 0.03920502391272924, Total loss: 0.004611101699993014, Count: 3\n",
      "Epoch [35/100], RMSE on validation data: 0.0425930012516785, Total loss: 0.005442491266876459, Count: 3\n",
      "Epoch [36/100], RMSE on validation data: 0.0340084369725178, Total loss: 0.0034697213559411466, Count: 3\n",
      "Epoch [37/100], RMSE on validation data: 0.023072518966832007, Total loss: 0.0015970233944244683, Count: 3\n",
      "Epoch [38/100], RMSE on validation data: 0.02791701779873834, Total loss: 0.0023380796483252198, Count: 3\n",
      "Epoch [39/100], RMSE on validation data: 0.05668229318812317, Total loss: 0.009638647083193064, Count: 3\n",
      "Epoch [40/100], RMSE on validation data: 0.07485606302636819, Total loss: 0.01681029051542282, Count: 3\n",
      "Epoch [41/100], RMSE on validation data: 0.07251152203420219, Total loss: 0.015773762483149767, Count: 3\n",
      "Epoch [42/100], RMSE on validation data: 0.05914528405933376, Total loss: 0.010494493879377842, Count: 3\n",
      "Epoch [43/100], RMSE on validation data: 0.038612442353197746, Total loss: 0.0044727621134370565, Count: 3\n",
      "Epoch [44/100], RMSE on validation data: 0.02116415885000322, Total loss: 0.0013437648594845086, Count: 3\n",
      "Epoch [45/100], RMSE on validation data: 0.03352389352105732, Total loss: 0.0033715543104335666, Count: 3\n",
      "Epoch [46/100], RMSE on validation data: 0.058243949811451944, Total loss: 0.010177073068916798, Count: 3\n",
      "Epoch [47/100], RMSE on validation data: 0.06841403161250355, Total loss: 0.014041439164429903, Count: 3\n",
      "Epoch [48/100], RMSE on validation data: 0.07129818387662029, Total loss: 0.01525029307231307, Count: 3\n",
      "Epoch [49/100], RMSE on validation data: 0.06073267625611396, Total loss: 0.011065373895689845, Count: 3\n",
      "Epoch [50/100], RMSE on validation data: 0.04092367543277739, Total loss: 0.005024241632781923, Count: 3\n",
      "Epoch [51/100], RMSE on validation data: 0.025579507246625686, Total loss: 0.0019629335729405284, Count: 3\n",
      "Epoch [52/100], RMSE on validation data: 0.025584000036815234, Total loss: 0.0019636231736512855, Count: 3\n",
      "Epoch [53/100], RMSE on validation data: 0.03772785918000133, Total loss: 0.004270174074918032, Count: 3\n",
      "Epoch [54/100], RMSE on validation data: 0.04614530156691527, Total loss: 0.006388166570104659, Count: 3\n",
      "Epoch [55/100], RMSE on validation data: 0.047874925871943974, Total loss: 0.006876025581732392, Count: 3\n",
      "Epoch [56/100], RMSE on validation data: 0.039229548913675166, Total loss: 0.004616872523911297, Count: 3\n",
      "Epoch [57/100], RMSE on validation data: 0.028180872878252907, Total loss: 0.0023824847885407507, Count: 3\n",
      "Epoch [58/100], RMSE on validation data: 0.0226391219715356, Total loss: 0.0015375895309261978, Count: 3\n",
      "Epoch [59/100], RMSE on validation data: 0.02218549942157109, Total loss: 0.0014765891537535936, Count: 3\n",
      "Epoch [60/100], RMSE on validation data: 0.02646582896051577, Total loss: 0.002101320307701826, Count: 3\n",
      "Epoch [61/100], RMSE on validation data: 0.030110221077498913, Total loss: 0.0027198762400075793, Count: 3\n",
      "Epoch [62/100], RMSE on validation data: 0.03135202375926299, Total loss: 0.0029488481814041734, Count: 3\n",
      "Epoch [63/100], RMSE on validation data: 0.03650675461029779, Total loss: 0.003998229396529496, Count: 3\n",
      "Epoch [64/100], RMSE on validation data: 0.035811010442153186, Total loss: 0.003847285406664014, Count: 3\n",
      "Epoch [65/100], RMSE on validation data: 0.026540383137438173, Total loss: 0.0021131758112460375, Count: 3\n",
      "Epoch [66/100], RMSE on validation data: 0.01849898858295234, Total loss: 0.0010266377357766032, Count: 3\n",
      "Epoch [67/100], RMSE on validation data: 0.020316207624547673, Total loss: 0.001238244876731187, Count: 3\n",
      "Epoch [68/100], RMSE on validation data: 0.02411510854022796, Total loss: 0.0017446153797209263, Count: 3\n",
      "Epoch [69/100], RMSE on validation data: 0.023678419502020674, Total loss: 0.001682002650341019, Count: 3\n",
      "Epoch [70/100], RMSE on validation data: 0.02130975231005643, Total loss: 0.0013623166305478662, Count: 3\n",
      "Epoch [71/100], RMSE on validation data: 0.018739856819618823, Total loss: 0.0010535467008594424, Count: 3\n",
      "Epoch [72/100], RMSE on validation data: 0.017930281984144498, Total loss: 0.0009644850360928103, Count: 3\n",
      "Epoch [73/100], RMSE on validation data: 0.019984184050536296, Total loss: 0.001198102836497128, Count: 3\n",
      "Epoch [74/100], RMSE on validation data: 0.025682535394467042, Total loss: 0.001978777872864157, Count: 3\n",
      "Epoch [75/100], RMSE on validation data: 0.025805113957080617, Total loss: 0.0019977117190137506, Count: 3\n",
      "Epoch [76/100], RMSE on validation data: 0.02304318564864128, Total loss: 0.0015929652145132422, Count: 3\n",
      "Epoch [77/100], RMSE on validation data: 0.01937240239164328, Total loss: 0.0011258699232712388, Count: 3\n",
      "Epoch [78/100], RMSE on validation data: 0.020293445860354677, Total loss: 0.001235471834661439, Count: 3\n",
      "Epoch [79/100], RMSE on validation data: 0.021899943884015425, Total loss: 0.001438822626369074, Count: 3\n",
      "Epoch [80/100], RMSE on validation data: 0.023408084654438624, Total loss: 0.001643815281568095, Count: 3\n",
      "Epoch [81/100], RMSE on validation data: 0.023990539612880966, Total loss: 0.0017266379727516323, Count: 3\n",
      "Epoch [82/100], RMSE on validation data: 0.023948929700098497, Total loss: 0.0017206537013407797, Count: 3\n",
      "Epoch [83/100], RMSE on validation data: 0.02534668644185014, Total loss: 0.001927363540744409, Count: 3\n",
      "Epoch [84/100], RMSE on validation data: 0.022751379614640802, Total loss: 0.0015528758231084794, Count: 3\n",
      "Epoch [85/100], RMSE on validation data: 0.023590742992863904, Total loss: 0.001669569464866072, Count: 3\n",
      "Epoch [86/100], RMSE on validation data: 0.025323543439720875, Total loss: 0.0019238455570302904, Count: 3\n",
      "Epoch [87/100], RMSE on validation data: 0.02601755200407574, Total loss: 0.0020307390368543565, Count: 3\n",
      "Epoch [88/100], RMSE on validation data: 0.02428610369304386, Total loss: 0.0017694444977678359, Count: 3\n",
      "Epoch [89/100], RMSE on validation data: 0.022421451644148833, Total loss: 0.001508164481492713, Count: 3\n",
      "Epoch [90/100], RMSE on validation data: 0.021922312787156256, Total loss: 0.0014417633938137442, Count: 3\n",
      "Epoch [91/100], RMSE on validation data: 0.025968078238190254, Total loss: 0.002023023262154311, Count: 3\n",
      "Epoch [92/100], RMSE on validation data: 0.03236779778849182, Total loss: 0.0031430230010300875, Count: 3\n",
      "Epoch [93/100], RMSE on validation data: 0.03550464007231369, Total loss: 0.0037817383999936283, Count: 3\n",
      "Epoch [94/100], RMSE on validation data: 0.03423987691858169, Total loss: 0.0035171075141988695, Count: 3\n",
      "Epoch [95/100], RMSE on validation data: 0.029898747065993814, Total loss: 0.0026818052283488214, Count: 3\n",
      "Epoch [96/100], RMSE on validation data: 0.023831766866766448, Total loss: 0.0017038593359757215, Count: 3\n",
      "Epoch [97/100], RMSE on validation data: 0.020560125252306577, Total loss: 0.0012681562511716038, Count: 3\n",
      "Epoch [98/100], RMSE on validation data: 0.021423277352179432, Total loss: 0.0013768704375252128, Count: 3\n",
      "Epoch [99/100], RMSE on validation data: 0.02402042423969438, Total loss: 0.0017309423419646919, Count: 3\n",
      "Epoch [100/100], RMSE on validation data: 0.024752091526087143, Total loss: 0.0018379981047473848, Count: 3\n"
     ]
    }
   ],
   "source": [
    "model_dict = train_model(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss on test data: 0.051572542171925306\n",
      "Total count on test data: 4\n",
      "RMSE on test data: 0.1135479438078089\n"
     ]
    }
   ],
   "source": [
    "r_model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "r_model.load_state_dict(model_dict)\n",
    "r_model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()  # Assuming you are still using MSE for evaluation\n",
    "total_loss = 0\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():  # Ensures that gradient computation is off, saving memory and computations\n",
    "    for bins, target in test_loader:\n",
    "        bins = bins.reshape(-1, sequence_length, input_size).to(device)\n",
    "        target = target.unsqueeze(1).to(device)\n",
    "        outputs = model(bins)\n",
    "        loss = criterion(outputs, target)\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "rmse = np.sqrt(total_loss / count)\n",
    "print('Total loss on test data:', total_loss)\n",
    "print('Total count on test data:', count)\n",
    "print(f'RMSE on test data: {rmse}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
