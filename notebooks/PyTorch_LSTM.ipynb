{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "\n",
    "from typing import Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('mps')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "with open('nb_parameters.yml') as file:\n",
    "    parameters = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "test_size = parameters['model_options']['test_size']\n",
    "\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper-parameters \n",
    "\n",
    "num_classes = parameters['model_options']['num_classes']\n",
    "num_epochs = parameters['model_options']['num_epochs']\n",
    "batch_size = parameters['model_options']['batch_size']\n",
    "learning_rate = parameters['model_options']['learning_rate']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Each feature as a time step in your sequence, you could set sequence_length to 150 and input_size to 1.\n",
    "This would mean you are feeding in sequences of length 150, with each time step in the sequence having 1 feature.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_size = parameters['model_options']['input_size']\n",
    "sequence_length = parameters['model_options']['sequence_length'] # the window it trains with can be selected\n",
    "hidden_size = parameters['model_options']['hidden_size']\n",
    "num_layers = parameters['model_options']['num_layers']\n",
    "random_state = parameters['model_options']['random_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My dataset for gas sensor\n",
    "df = pd.read_csv('../data/03_primary/model_input_table.csv')\n",
    "\n",
    "# read data in 03primary model_inpput_table.pq\n",
    "# df = pd.read_parquet('../data/03_primary/model_input_table.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(model_input_table: pd.DataFrame) -> torch.tensor:\n",
    "    # print(f\"Test size: {parameters['test_size']}, type: {type(parameters['test_size'])}\")\n",
    "    # print(f\"Random state: {parameters['random_state']}, type: {type(parameters['random_state'])}\")\n",
    "\n",
    "    # Split data into features and target\n",
    "    X = model_input_table[model_input_table.columns[:-1]].values  # Assuming last column is the target\n",
    "    y = model_input_table[model_input_table.columns[-1]].values\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training data\n",
    "    scaler.fit(X_train)\n",
    "    # Transform both training and testing data\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Ensure y_train and y_test are in the correct format\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train = y_train.values\n",
    "    if isinstance(y_test, pd.Series):\n",
    "        y_test = y_test.values\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled.astype(np.float32))\n",
    "    y_train_tensor = torch.tensor(y_train.astype(np.float32))\n",
    "    X_test_tensor = torch.tensor(X_test_scaled.astype(np.float32))\n",
    "    y_test_tensor = torch.tensor(y_test.astype(np.float32))\n",
    "\n",
    "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor from split_data(df)\n",
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = split_data(df)\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# Initialize DataLoaders\n",
    "batch_size = parameters['model_options']['batch_size']  # You can adjust the batch size according to your needs\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Now, train_loader and test_loader can be used in your training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the block below into a function\n",
    "def train_model (data: DataLoader)->():\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "    n_total_steps = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (bins, target) in enumerate(train_loader):  \n",
    "            bins = bins.reshape(-1, sequence_length, input_size).to(device)\n",
    "            target = target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(bins)\n",
    "        # Example of reshaping/squeezing if applicable\n",
    "        outputs = outputs.squeeze()  # Removes dimensions of size 1\n",
    "        outputs = outputs[:64]  # Adjust if you need to slice the outputs\n",
    "\n",
    "        target = target.unsqueeze(1).to(device)  # Add an extra dimension to match outputs\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Calculate RMSE at the end of each epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Don't calculate gradients\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for bins, target in test_loader:  # Replace with your validation loader\n",
    "                bins = bins.reshape(-1, sequence_length, input_size).to(device)\n",
    "                target = target.unsqueeze(1).to(device)  # Add an extra dimension to match outputs\n",
    "                outputs = model(bins)\n",
    "                loss = criterion(outputs, target)\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "            rmse = np.sqrt(total_loss / count)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], RMSE on validation data: {rmse}')\n",
    "        model.train()  # Set the model back to training mode\n",
    "    # Save the model after training\n",
    "    lstm_model = torch.save(model.state_dict())\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kpt/.pyenv/versions/3.10.13/envs/kedro-env/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], RMSE on validation data: 0.19470462299237312\n",
      "Epoch [2/10], RMSE on validation data: 0.17271015611667567\n",
      "Epoch [3/10], RMSE on validation data: 0.11168149516464093\n",
      "Epoch [4/10], RMSE on validation data: 0.10652974043979811\n",
      "Epoch [5/10], RMSE on validation data: 0.13228593485973064\n",
      "Epoch [6/10], RMSE on validation data: 0.13436266413702383\n",
      "Epoch [7/10], RMSE on validation data: 0.10969141055424163\n",
      "Epoch [8/10], RMSE on validation data: 0.08177059063703315\n",
      "Epoch [9/10], RMSE on validation data: 0.0870909988296493\n",
      "Epoch [10/10], RMSE on validation data: 0.10273631967944948\n",
      "Model saved to ../data/06_models/model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/06_models/model.pth'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3896]])\n"
     ]
    }
   ],
   "source": [
    "# model inference below\n",
    "\n",
    "inf_model = RNN(input_size=parameters['model_options']['input_size'], \n",
    "                  hidden_size=parameters['model_options']['hidden_size'], \n",
    "                  num_layers=parameters['model_options']['num_layers'], \n",
    "                  num_classes=parameters['model_options']['num_classes'])\n",
    "\n",
    "inf_model.load_state_dict(torch.load('../data/06_models/model.pth'))\n",
    "inf_model.eval()\n",
    "\n",
    "# Example of a dummy input (replace with actual data as needed)\n",
    "dummy_input = torch.randn(1, parameters['model_options']['sequence_length'], parameters['model_options']['input_size'])  # Shape: [batch_size, sequence_length, input_size]\n",
    "\n",
    "# Determine the device where the model is\n",
    "device = next(inf_model.parameters()).device\n",
    "\n",
    "# Move the dummy_input to the same device as the model\n",
    "dummy_input = dummy_input.to(device)\n",
    "\n",
    "# Now you can pass the dummy_input to the model\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    prediction = inf_model(dummy_input)\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
