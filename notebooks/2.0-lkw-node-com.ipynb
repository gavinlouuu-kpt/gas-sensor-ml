{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kedro.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext kedro.ipython\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import yaml\n",
    "\n",
    "from typing import Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "%load_ext kedro.ipython\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write functions to check if each node produces valid data that are suitable for next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to combine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data between exp_no 104 to 113\n",
    "# append data of 105 at the end of 104 etc\n",
    "\n",
    "def get_data(exp_no) -> pd.DataFrame:\n",
    "    file_name = f\"{exp_no}_SHT_SMD.txt\"\n",
    "    file_path = f\"../data/01_raw/{file_name}\"\n",
    "    df = pd.read_csv(file_path, sep=',', usecols=['timestamp', 'SHT40_temp', 'SHT40_Humidity', 'A1_Sensor', 'A1_Resistance'])\n",
    "    return df\n",
    "\n",
    "def concat_data(start:int,end:int) -> pd.DataFrame:\n",
    "    df = pd.concat([get_data(exp_no) for exp_no in range(start, end)])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "s_file = 108\n",
    "e_file = 113\n",
    "\n",
    "concat_data = concat_data(s_file,e_file)\n",
    "# concat_data(s_file,e_file).to_parquet(f'../data/02_intermediate/{s_file}_{e_file}.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hi_lo_peak(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    peaks, properties = find_peaks(x['A1_Sensor'], width=50, height=1)\n",
    "    peak_heights = properties['peak_heights']\n",
    "# Determine smaller and larger peaks\n",
    "    smaller_peaks, larger_peaks = [], []\n",
    "    for i in range(len(peaks) - 1):\n",
    "        if peak_heights[i] > peak_heights[i + 1]:\n",
    "            larger_peaks.append(peaks[i])\n",
    "            smaller_peaks.append(peaks[i + 1])\n",
    "    # smaller_peaks_df = x.iloc[smaller_peaks]\n",
    "    return smaller_peaks\n",
    "\n",
    "def data_stack(sp: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    After finding the peaks, stack the data according to exp_no\n",
    "    \"\"\"\n",
    "    df_stacked_list = []\n",
    "    for i in range(len(sp) - 1):\n",
    "        df_subset = df.iloc[sp[i]:sp[i + 1]].copy()\n",
    "        df_subset['exp_no'] = i\n",
    "        df_subset['timestamp'] -= df_subset['timestamp'].iloc[0]\n",
    "        df_stacked_list.append(df_subset)\n",
    "        df_stacked = pd.concat(df_stacked_list, ignore_index=True)\n",
    "    return df_stacked\n",
    "\n",
    "\n",
    "def _group_by_bin(df_stacked: pd.DataFrame, num_bins: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use PD.CUT to group data into specified bins in parameters\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    grouped = df_stacked.groupby('exp_no')\n",
    "    for name, group in grouped:\n",
    "        group['bin'] = pd.cut(group['timestamp'], bins=num_bins, labels=False)\n",
    "        df_list.append(group)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def _average_bin(bin_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    average values within each bin to return only one data point\n",
    "    \"\"\"\n",
    "    bin_df = bin_df.drop(columns=['timestamp'])\n",
    "    grouped = bin_df.groupby(['exp_no', 'bin']).mean()\n",
    "    return grouped.reset_index()\n",
    "\n",
    "def preprocess_data_bin(mox: pd.DataFrame, num_bins: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return data that is sorted by experiment number according to lo_peak interval\n",
    "    data is stacked and labeled by exp_no\n",
    "    data is grouped by bin and averaged\n",
    "    \"\"\"\n",
    "    df_stacked = data_stack(_hi_lo_peak(mox), mox)\n",
    "    bin_df = _group_by_bin(df_stacked, num_bins)\n",
    "    mean_bin = _average_bin(bin_df)\n",
    "    return mean_bin\n",
    "\n",
    "def get_percentile_data(df, percentile):\n",
    "    \"\"\"\n",
    "    Returns the data up to the specified percentile based on the 'bin' column.\n",
    "\n",
    "    :param df: DataFrame containing the data\n",
    "    :param percentile: A float value between 0 and 1 representing the percentile\n",
    "    :return: DataFrame containing the data up to the specified percentile\n",
    "    \"\"\"\n",
    "    # Calculate the bin index corresponding to the percentile\n",
    "    max_bin = int(percentile * df['bin'].max())\n",
    "\n",
    "    # Return data up to that bin\n",
    "    return df[df['bin'] <= max_bin]\n",
    "\n",
    "def _group_percentile (averaged: pd.DataFrame, percentile_bins: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns the full specified percentile dataset\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    grouped = averaged.groupby('exp_no')\n",
    "    for name, group in grouped:\n",
    "        percentile_data = get_percentile_data(group, percentile_bins)\n",
    "        df_list.append(percentile_data)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def _transpose_(df_set: pd.DataFrame) -> pd.DataFrame:\n",
    "    transposed = df_set.pivot(index='exp_no', columns='bin', values='A1_Resistance')\n",
    "    transposed.columns = ['bin_' + str(col) for col in transposed.columns]\n",
    "    transposed.reset_index(inplace=True)\n",
    "    return transposed\n",
    "\n",
    "\n",
    "def _res_ratio(averaged: pd.DataFrame) -> pd.DataFrame:\n",
    "    def calculate_res_ratio(group):\n",
    "        return group['A1_Resistance'].max() / group['A1_Resistance'].min()\n",
    "\n",
    "    res_ratio = averaged.groupby('exp_no').apply(calculate_res_ratio).reset_index()\n",
    "    res_ratio.columns = ['exp_no', 'res_ratio']\n",
    "    return res_ratio\n",
    "\n",
    "def _combine_feature_matrix(res_ratio: pd.DataFrame, transposed: pd.DataFrame) -> pd.DataFrame:\n",
    "    combined = pd.merge(res_ratio, transposed, on='exp_no')\n",
    "    return combined\n",
    "\n",
    "def create_model_input_table(mox_bin: pd.DataFrame, percentile_bins: float) -> pd.DataFrame:\n",
    "    selected_range = _group_percentile(mox_bin, percentile_bins)\n",
    "    # the ratio is from the entire dataset not filtered to be ground truth\n",
    "    res_ratio = _res_ratio(mox_bin) \n",
    "    transpose_col = _transpose_(selected_range)\n",
    "    # drop exp_no to avoid training on exp_no\n",
    "    mox_table = _combine_feature_matrix(transpose_col, res_ratio).drop(columns=['exp_no'])\n",
    "    return mox_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "with open('nb_parameters.yml') as file:\n",
    "    parameters = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "test_size = parameters['model_options']['test_size']\n",
    "\n",
    "print(test_size)\n",
    "\n",
    "\n",
    "# Hyper-parameters \n",
    "\n",
    "num_classes = parameters['model_options']['num_classes']\n",
    "num_epochs = parameters['model_options']['num_epochs']\n",
    "batch_size = parameters['model_options']['batch_size']\n",
    "learning_rate = parameters['model_options']['learning_rate']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Each feature as a time step in your sequence, you could set sequence_length to 150 and input_size to 1.\n",
    "This would mean you are feeding in sequences of length 150, with each time step in the sequence having 1 feature.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_size = parameters['model_options']['input_size']\n",
    "sequence_length = parameters['model_options']['sequence_length'] # the window it trains with can be selected\n",
    "hidden_size = parameters['model_options']['hidden_size']\n",
    "num_layers = parameters['model_options']['num_layers']\n",
    "random_state = parameters['model_options']['random_state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Process and examine each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_no = 107\n",
    "percentile_bins = parameters['percentile_bins']\n",
    "bin_size = parameters['num_bins']\n",
    "\n",
    "df_exp = get_data(exp_no)\n",
    "smaller_peaks = _hi_lo_peak(df_exp)\n",
    "df_stacked = data_stack(smaller_peaks, df_exp)\n",
    "bin_df = _group_by_bin(df_stacked, bin_size)\n",
    "mean_bin = _average_bin(bin_df)\n",
    "mox_bin = preprocess_data_bin(df_exp, bin_size)\n",
    "selected_range = _group_percentile(mox_bin, percentile_bins)\n",
    "res_ratio = _res_ratio(mox_bin)\n",
    "transpose_col = _transpose_(selected_range)\n",
    "mox_table = _combine_feature_matrix(transpose_col, res_ratio).drop(columns=['exp_no'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        timestamp  SHT40_temp  SHT40_Humidity  A1_Sensor  A1_Resistance  \\\n",
      "0               0       28.69           41.54     1334.0     2798725.50   \n",
      "1              50       28.66           41.61     1333.0     2800937.50   \n",
      "2             100       28.66           41.66     1334.0     2798725.50   \n",
      "3             150       28.68           41.74     1331.0     2805372.00   \n",
      "4             200       28.67           41.78     1332.0     2803153.00   \n",
      "...           ...         ...             ...        ...            ...   \n",
      "881871     181650       27.07           44.59      970.0     3905257.75   \n",
      "881872     181701       27.07           44.63      970.0     3905257.75   \n",
      "881873     181751       27.09           44.66      970.0     3905257.75   \n",
      "881874     181801       27.09           44.68      968.0     3913636.25   \n",
      "881875     181851       27.10           44.74      969.0     3909442.75   \n",
      "\n",
      "        exp_no   bin  \n",
      "0            0     0  \n",
      "1            0     0  \n",
      "2            0     0  \n",
      "3            0     1  \n",
      "4            0     1  \n",
      "...        ...   ...  \n",
      "881871     243  1498  \n",
      "881872     243  1498  \n",
      "881873     243  1499  \n",
      "881874     243  1499  \n",
      "881875     243  1499  \n",
      "\n",
      "[881876 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(bin_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LSTM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement LSTM functions below\n",
    "# there is no validation set in this example\n",
    "# load mox_table as input\n",
    "\n",
    "def split_data(model_input_table: pd.DataFrame) -> torch.tensor:\n",
    "    # print(f\"Test size: {parameters['test_size']}, type: {type(parameters['test_size'])}\")\n",
    "    # print(f\"Random state: {parameters['random_state']}, type: {type(parameters['random_state'])}\")\n",
    "\n",
    "    # Split data into features and target\n",
    "    X = model_input_table[model_input_table.columns[:-1]].values  # Assuming last column is the target\n",
    "    y = model_input_table[model_input_table.columns[-1]].values\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training data\n",
    "    scaler.fit(X_train)\n",
    "    # Transform both training and testing data\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Ensure y_train and y_test are in the correct format\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train = y_train.values\n",
    "    if isinstance(y_test, pd.Series):\n",
    "        y_test = y_test.values\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled.astype(np.float32))\n",
    "    y_train_tensor = torch.tensor(y_train.astype(np.float32))\n",
    "    X_test_tensor = torch.tensor(X_test_scaled.astype(np.float32))\n",
    "    y_test_tensor = torch.tensor(y_test.astype(np.float32))\n",
    "\n",
    "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
    "\n",
    "# create X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor from split_data(df)\n",
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = split_data(mox_table)\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# Initialize DataLoaders\n",
    "batch_size = parameters['model_options']['batch_size']  # You can adjust the batch size according to your needs\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Now, train_loader and test_loader can be used in your training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the block below into a function\n",
    "def train_model (data: DataLoader)->():\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "    n_total_steps = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (bins, target) in enumerate(train_loader):  \n",
    "            bins = bins.reshape(-1, sequence_length, input_size).to(device)\n",
    "            target = target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(bins)\n",
    "        # Example of reshaping/squeezing if applicable\n",
    "        outputs = outputs.squeeze()  # Removes dimensions of size 1\n",
    "        outputs = outputs[:64]  # Adjust if you need to slice the outputs\n",
    "\n",
    "        target = target.unsqueeze(1).to(device)  # Add an extra dimension to match outputs\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Calculate RMSE at the end of each epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Don't calculate gradients\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for bins, target in test_loader:  # Replace with your validation loader\n",
    "                bins = bins.reshape(-1, sequence_length, input_size).to(device)\n",
    "                target = target.unsqueeze(1).to(device)  # Add an extra dimension to match outputs\n",
    "                outputs = model(bins)\n",
    "                loss = criterion(outputs, target)\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "            rmse = np.sqrt(total_loss / count)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], RMSE on validation data: {rmse}')\n",
    "        model.train()  # Set the model back to training mode\n",
    "    # Save the model after training\n",
    "    # lstm_model = torch.save(model.state_dict())\n",
    "    lstm_model = model.state_dict()\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/25/23 15:54:56] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> <span style=\"color: #800080; text-decoration-color: #800080\">/Users/kpt/.pyenv/versions/3.10.13/envs/kedro-env/lib/python3.10/site-</span> <a href=\"file:///Users/kpt/.pyenv/versions/3.10.13/lib/python3.10/warnings.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">warnings.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kpt/.pyenv/versions/3.10.13/lib/python3.10/warnings.py#109\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">packages/torch/nn/modules/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">loss.py</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">535</span>: UserWarning: Using a target     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         size <span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]))</span> that is different to the input size          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]))</span>. This will likely lead to incorrect results due to   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         broadcasting. Please ensure they have the same size.                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>           return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">F.mse_loss</span><span style=\"font-weight: bold\">(</span>input, target, <span style=\"color: #808000; text-decoration-color: #808000\">reduction</span>=<span style=\"color: #800080; text-decoration-color: #800080\">self</span>.reduction<span style=\"font-weight: bold\">)</span>           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/25/23 15:54:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m \u001b[35m/Users/kpt/.pyenv/versions/3.10.13/envs/kedro-env/lib/python3.10/site-\u001b[0m \u001b]8;id=119075;file:///Users/kpt/.pyenv/versions/3.10.13/lib/python3.10/warnings.py\u001b\\\u001b[2mwarnings.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=446894;file:///Users/kpt/.pyenv/versions/3.10.13/lib/python3.10/warnings.py#109\u001b\\\u001b[2m109\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35mpackages/torch/nn/modules/\u001b[0m\u001b[95mloss.py\u001b[0m:\u001b[1;36m535\u001b[0m: UserWarning: Using a target     \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         size \u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m that is different to the input size          \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m. This will likely lead to incorrect results due to   \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         broadcasting. Please ensure they have the same size.                   \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m           return \u001b[1;35mF.mse_loss\u001b[0m\u001b[1m(\u001b[0minput, target, \u001b[33mreduction\u001b[0m=\u001b[35mself\u001b[0m.reduction\u001b[1m)\u001b[0m           \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], RMSE on validation data: 1.3545067201259686\n",
      "Epoch [2/100], RMSE on validation data: 1.3204957394724373\n",
      "Epoch [3/100], RMSE on validation data: 1.2841561202871044\n",
      "Epoch [4/100], RMSE on validation data: 1.243695189835406\n",
      "Epoch [5/100], RMSE on validation data: 1.1975637690104044\n",
      "Epoch [6/100], RMSE on validation data: 1.1410319895551913\n",
      "Epoch [7/100], RMSE on validation data: 1.0684854966501616\n",
      "Epoch [8/100], RMSE on validation data: 0.9692969623933668\n",
      "Epoch [9/100], RMSE on validation data: 0.8238492743217882\n",
      "Epoch [10/100], RMSE on validation data: 0.5953366887942981\n",
      "Epoch [11/100], RMSE on validation data: 0.20842073651058673\n",
      "Epoch [12/100], RMSE on validation data: 0.5266625914825946\n",
      "Epoch [13/100], RMSE on validation data: 0.5831717192056292\n",
      "Epoch [14/100], RMSE on validation data: 0.3939146520724038\n",
      "Epoch [15/100], RMSE on validation data: 0.1568509822674554\n",
      "Epoch [16/100], RMSE on validation data: 0.13019104800628117\n",
      "Epoch [17/100], RMSE on validation data: 0.23983998023869732\n",
      "Epoch [18/100], RMSE on validation data: 0.31121226467856516\n",
      "Epoch [19/100], RMSE on validation data: 0.3472747745746059\n",
      "Epoch [20/100], RMSE on validation data: 0.3556002934166695\n",
      "Epoch [21/100], RMSE on validation data: 0.3328879048214625\n",
      "Epoch [22/100], RMSE on validation data: 0.29213309635769535\n",
      "Epoch [23/100], RMSE on validation data: 0.2370893891032657\n",
      "Epoch [24/100], RMSE on validation data: 0.17239792478340554\n",
      "Epoch [25/100], RMSE on validation data: 0.10394404260016632\n",
      "Epoch [26/100], RMSE on validation data: 0.051338229779580725\n",
      "Epoch [27/100], RMSE on validation data: 0.0719898008710587\n",
      "Epoch [28/100], RMSE on validation data: 0.1210601860107353\n",
      "Epoch [29/100], RMSE on validation data: 0.16650194575254665\n",
      "Epoch [30/100], RMSE on validation data: 0.1917614026101712\n",
      "Epoch [31/100], RMSE on validation data: 0.19578238469789794\n",
      "Epoch [32/100], RMSE on validation data: 0.19268104992646115\n",
      "Epoch [33/100], RMSE on validation data: 0.1718286037137477\n",
      "Epoch [34/100], RMSE on validation data: 0.13965595831993152\n",
      "Epoch [35/100], RMSE on validation data: 0.10208322497547814\n",
      "Epoch [36/100], RMSE on validation data: 0.06769404285344184\n",
      "Epoch [37/100], RMSE on validation data: 0.04971547411056444\n",
      "Epoch [38/100], RMSE on validation data: nan\n",
      "Epoch [39/100], RMSE on validation data: nan\n",
      "Epoch [40/100], RMSE on validation data: nan\n",
      "Epoch [41/100], RMSE on validation data: nan\n",
      "Epoch [42/100], RMSE on validation data: nan\n",
      "Epoch [43/100], RMSE on validation data: nan\n",
      "Epoch [44/100], RMSE on validation data: nan\n",
      "Epoch [45/100], RMSE on validation data: nan\n",
      "Epoch [46/100], RMSE on validation data: nan\n",
      "Epoch [47/100], RMSE on validation data: nan\n",
      "Epoch [48/100], RMSE on validation data: nan\n",
      "Epoch [49/100], RMSE on validation data: nan\n",
      "Epoch [50/100], RMSE on validation data: nan\n",
      "Epoch [51/100], RMSE on validation data: nan\n",
      "Epoch [52/100], RMSE on validation data: nan\n",
      "Epoch [53/100], RMSE on validation data: nan\n",
      "Epoch [54/100], RMSE on validation data: nan\n",
      "Epoch [55/100], RMSE on validation data: nan\n",
      "Epoch [56/100], RMSE on validation data: nan\n",
      "Epoch [57/100], RMSE on validation data: nan\n",
      "Epoch [58/100], RMSE on validation data: nan\n",
      "Epoch [59/100], RMSE on validation data: nan\n",
      "Epoch [60/100], RMSE on validation data: nan\n",
      "Epoch [61/100], RMSE on validation data: nan\n",
      "Epoch [62/100], RMSE on validation data: nan\n",
      "Epoch [63/100], RMSE on validation data: nan\n",
      "Epoch [64/100], RMSE on validation data: nan\n",
      "Epoch [65/100], RMSE on validation data: nan\n",
      "Epoch [66/100], RMSE on validation data: nan\n",
      "Epoch [67/100], RMSE on validation data: nan\n",
      "Epoch [68/100], RMSE on validation data: nan\n",
      "Epoch [69/100], RMSE on validation data: nan\n",
      "Epoch [70/100], RMSE on validation data: nan\n",
      "Epoch [71/100], RMSE on validation data: nan\n",
      "Epoch [72/100], RMSE on validation data: nan\n",
      "Epoch [73/100], RMSE on validation data: nan\n",
      "Epoch [74/100], RMSE on validation data: nan\n",
      "Epoch [75/100], RMSE on validation data: nan\n",
      "Epoch [76/100], RMSE on validation data: nan\n",
      "Epoch [77/100], RMSE on validation data: nan\n",
      "Epoch [78/100], RMSE on validation data: nan\n",
      "Epoch [79/100], RMSE on validation data: nan\n",
      "Epoch [80/100], RMSE on validation data: nan\n",
      "Epoch [81/100], RMSE on validation data: nan\n",
      "Epoch [82/100], RMSE on validation data: nan\n",
      "Epoch [83/100], RMSE on validation data: nan\n",
      "Epoch [84/100], RMSE on validation data: nan\n",
      "Epoch [85/100], RMSE on validation data: nan\n",
      "Epoch [86/100], RMSE on validation data: nan\n",
      "Epoch [87/100], RMSE on validation data: nan\n",
      "Epoch [88/100], RMSE on validation data: nan\n",
      "Epoch [89/100], RMSE on validation data: nan\n",
      "Epoch [90/100], RMSE on validation data: nan\n",
      "Epoch [91/100], RMSE on validation data: nan\n",
      "Epoch [92/100], RMSE on validation data: nan\n",
      "Epoch [93/100], RMSE on validation data: nan\n",
      "Epoch [94/100], RMSE on validation data: nan\n",
      "Epoch [95/100], RMSE on validation data: nan\n",
      "Epoch [96/100], RMSE on validation data: nan\n",
      "Epoch [97/100], RMSE on validation data: nan\n",
      "Epoch [98/100], RMSE on validation data: nan\n",
      "Epoch [99/100], RMSE on validation data: nan\n",
      "Epoch [100/100], RMSE on validation data: nan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mOrderedDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'lstm.weight_ih_l0'\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'mps:0'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'lstm.weight_hh_l0'\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[33m...\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'mps:0'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'lstm.bias_ih_l0'\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'mps:0'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'lstm.bias_hh_l0'\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'mps:0'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'lstm.weight_ih_l1'\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[33m...\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'mps:0'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'lstm.weight_hh_l1'\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[33m...\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0mnan, nan, nan,  \u001b[33m...\u001b[0m, nan, nan, nan\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'mps:0'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'lstm.bias_ih_l1'\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'mps:0'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'lstm.bias_hh_l1'\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'mps:0'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'fc.weight'\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0mnan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'mps:0'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'fc.bias'\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0mnan\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'mps:0'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
